import networkx as nx
import time
from src.graph.graph import Graph
from src.logging import *
from src.evaluation.newEvaluationMethod import newEvaluationMethod
import functions
from datetime import timedelta
import numpy as np
import pickle

start_time = time.time()

G = nx.DiGraph()

print('Path Mining Tool Demo by USF SEES Lab')
print()

if __name__ == '__main__':

    max_pat_len = 8
    max_solutions = 10
    def_f = ""
    trace_f = ""

    # Uncomment corresponding lines to genearte solutions for different traces

    # For gem5 traces

    # Full system (FS) simulation traces
    # def_f = './traces/gem5_traces/fs/definition/fs_def.msg'
    # def_f = './traces/gem5_traces/fs/definition/def-FS-RublePrintFormat.msg'
    # fs unsliced
    # trace_f = ['./traces/gem5_traces/fs/unsliced/unsliced0.jbl']
    # trace_f = ['./traces/gem5_traces/fs/unsliced/fs_boot_unsliced.txt']
    # fs packet id sliced
    # trace_f = ['./traces/gem5_traces/fs/packet_sliced/packet_sliced.jbl']
    # fs memory address sliced
    # trace_f = ['./traces/gem5_traces/fs/addr_sliced/address_sliced_no_duplicates.jbl']

    # Snoop (SE) traces
    # def_f = './traces/gem5_traces/snoop/definition/paterson_def.msg'
    # def_f = './traces/gem5_traces/snoop/definition/renamed.msg'
    # snoop unsliced
    # trace_f = ['./traces/gem5_traces/snoop/unsliced/paterson_unsliced.txt']
    # snoop packet id sliced
    # trace_f = ['./traces/gem5_traces/snoop/packet_sliced/packet_sliced.jbl']
    # snoop memory address sliced
    # trace_f = ['./traces/gem5_traces/snoop/addr_sliced/address_sliced.jbl']

    # Threads (SE) traces
    # def_f = './traces/gem5_traces/threads/definition/threads_def.msg' # This definition file doesn't include any initial or terminal nodes
    # def_f = './traces/gem5_traces/threads/definition/definition.txt'  
    # def_f = './traces/gem5_traces/threads/definition/renamedDefinitionFile.msg'  
    # def_f = './traces/gem5_traces/threads/definition/myDefinition.txt'  
    # def_f = './traces/gem5_traces/threads/definition/newDefinition.txt' 
    # def_f = './traces/gem5_traces/threads/definition/generatedByMeDef.msg'  
    # def_f = './traces/gem5_traces/threads/definition/TestDef.msg'  # Threads Final Definition file generated By Bardia
    # threads unsliced
    # trace_f = ['./traces/gem5_traces/threads/unsliced/unsliced.txt']
    # trace_f = ['./traces/gem5_traces/threads/unsliced/testTrace.txt']   # Threads Final Trace file generated By Bardia
    # trace_f = ['./traces/gem5_traces/threads/unsliced/generatedByMeThreadsTraceFile.txt']
    # threads packet id sliced
    # trace_f = ['./traces/gem5_traces/threads/packet_sliced/packet_sliced.jbl']   
    # snoop memory address sliced
    # trace_f = ['./traces/gem5_traces/threads/addr_sliced/address_sliced.jbl']
    # trace_f = ['./traces/gem5_traces/threads/addr_sliced/address_sliced_compact.jbl']


    # For synthetic traces
    def_f = './traces/synthetic/newLarge.msg'
    # def_f = './traces/synthetic/large.msg'
    # def_f = './traces/synthetic/medium.msg'
    # def_f = './traces/synthetic/small.msg'
    # def_f = './traces/synthetic/Test/testDefinition.msg'

    # small traces
    # trace_f = ['./traces/synthetic/trace-small-5.txt']
    # trace_f = ['./traces/synthetic/trace-small-10.txt']
    # trace_f = ['./traces/synthetic/trace-small-20.txt']
    # trace_f = ['./traces/synthetic/trace-small-5-New.txt']
    # trace_f = ['./traces/synthetic/trace-small-test.txt']
    # trace_f = ['./traces/synthetic/trace-small-test2.txt']
    # trace_f = ['./traces/synthetic/trace-small-test3.txt']

    # large traces
    # trace_f = ['./traces/synthetic/trace-large-5.txt']
    trace_f = ['./traces/synthetic/trace-large-10.txt']
    # trace_f = ['./traces/synthetic/trace-large-20.txt']
    # trace_f = ['./traces/synthetic/new-trace-large-20.txt']

    # trace_f = ['./traces/synthetic/testCode.txt']
    # trace_f = ['./traces/synthetic/Test/testTrace.txt']

    # For Testing 
    # def_f = './traces/ForTest/testDefinitionFile.txt'
    # trace_f = ['./traces/ForTest/testTrace.txt']
    # def_f = './traces/ForTest/L3cacheTestDefinitionFile.txt'
    # trace_f = ['./traces/ForTest/L3cacheTestTrace.txt']

    # From TCAD Paper
    # def_f   = './traces/fromTCAD/gem5/threads/definition/defThreads-RubelPrintFormat.msg'
    # trace_f = ['./traces/fromTCAD/gem5/threads/architecturalSliced/totalSliced.jbl']

    def_f   = './traces/fromTCAD/gem5/snoop/definition/defSnoop-RubelPrintFormat.msg'
    # trace_f = ['./traces/fromTCAD/gem5/snoop/architecturalSliced/totalSliced.jbl']
    trace_f = ['./traces/fromTCAD/gem5/snoop/unsliced/unsliced-RubelPrintFormat.jbl']
    # trace_f = ['buggy-trace.txt']

    # def_f   = './traces/fromTCAD/gem5/threads/definition/defThreads-RubelPrintFormat.msg'
    # trace_f = ['./traces/fromTCAD/gem5/threads/unsliced/unsliced-RubelPrintFormat.jbl']


    # essential_mode = False
    # essential_edges_array = []
    # if (essential_mode == True):
    #     print("Finding Essentails!")
    #     essential_edges_array = src.essential.EssentialsEfficient.find_essential_causalities(def_f, trace_f[0])
    # print (essential_edges_array)
    # print("Finding Essentails: Done!")

    # traceType = "synthetic"
    # traceType = "gem5"
    filters_filename = None
    rank_filename    = None

    graph = Graph()
    graph.set_max_height(max_pat_len)
    graph.set_max_solutions(max_solutions)

    
    if "gem5" in def_f:
        ####################### For Threads
        graph.window = False
    else:
        ####################### For Large20
        graph.window = False
    graph.window_size = 50

    if (graph.window and (graph.window <= 0)):
        print("Winodw size must > 0")
        exit()
    if(graph.window):
        print("Added window slicing...window size: ", graph.window_size)
        print()

    log('Reading the message definition file %s... ' % def_f)
    if def_f=="":
        exit()
    graph.read_message_file(def_f)
    log('Done\n\n')

    traces = None
    log('Reading the trace file(s) %s... ' % trace_f)
    graph.read_trace_file_list(trace_f)
    log('Trace reading and processing status: Done\n\n')

    ######################################################################## Start ########################################################################

    maxNodeNumber = 0
    for i in graph.nodes:
        if maxNodeNumber < int(i):
            maxNodeNumber = int(i)

    adjMatrix = [[0 for i in range(maxNodeNumber+1)] for j in range(maxNodeNumber+1)]
    print("My initial nodes: ", graph.myInitialNodes)
    print("My terminal nodes: ", graph.myTerminalNodes)

    f = open("localPatrerns.txt", "a")
    localPatternCounter = 0
    for i in graph.nodes:
        print(i, ":", end=" ")
        for n in graph.networkxGraph.neighbors(int(i)):
            print(n, end=" ")
            if int(i) in graph.myInitialNodes and int(n) in graph.myTerminalNodes:
                adjMatrix[int(i)][int(n)] = 2
            else:
                adjMatrix[int(i)][int(n)] = 1
            f.write(str(i) + "_" + str(n) + "\n ")
            localPatternCounter += 1
        print()
    print("Total Local Patterns: ", localPatternCounter)
    f.close()

    counterTest = 0
    specialCounter = 0
    newReducedTrace = []
    print("Number of tokens (messages): ", len(graph.trace_tokens))
    # print(graph.trace_tokens)
    # exit()
    j = 0
    # for i in range(len(graph.trace_tokens)):
    while j < len(graph.trace_tokens):
        if j == len(graph.trace_tokens)-1:
            newReducedTrace.append(graph.trace_tokens[j])
            break
        if adjMatrix[int(graph.trace_tokens[j])][int(graph.trace_tokens[j+1])] == 2:
            specialCounter += 2
            j += 2
        # elif adjMatrix[int(graph.trace_tokens[j])][int(graph.trace_tokens[j+1])] == 1:
        #     newReducedTrace.append("A")
        #     counterTest += 1
        #     j += 2
        #     # i += 2
        else:
            newReducedTrace.append(graph.trace_tokens[j])
            j += 1

    print("Number of tokens in the reduced trace: ", len(newReducedTrace))
    print("Counter test =", counterTest)
    print("Special counter =", specialCounter)

    f = open("reducedTrace.txt", "w")
    for i in newReducedTrace:
        f.write(str(i) + " ")
    f.close()
    ######################################################################## END ########################################################################

    print(len(graph.edges))
    # exit() 

    ######################################################################## Start ########################################################################
    all_paths_sorted = [[] for x in range(graph.maxInitials+1)]  # = []
    selected_paths   = [[] for x in range(graph.maxInitials+1)] 


    if "gem5" in def_f:
        pruned_graph = functions.pruningGraph(graph.networkxGraph, graph, "gem5")
    else:
        pruned_graph = functions.pruningGraph(graph.networkxGraph, graph, "synthetic")

    # print("Starting nodes = ", graph.myInitialNodes)
    # print("Ending nodes = ",   graph.myTerminalNodes, "\n")
    all_paths_sorted = functions.pathPoolFinder(graph, pruned_graph, graph.myInitialNodes, graph.finalCorrespondingTerminalArray)
    
    selected_paths   = functions.modelSelector(all_paths_sorted, graph)

    numberOfPaths = 0
    for i in range(len(all_paths_sorted)):
        numberOfPaths += len(all_paths_sorted[i])
    print("Number of paths: ", len(all_paths_sorted))
    print("Number of total paths: ", numberOfPaths)
    # print("all paths sorted: ", all_paths_sorted)
    # all_sorted_paths_np_array = np.array(all_paths_sorted)
    # np.save("my_array.npy", all_sorted_paths_np_array) 

    # loaded_arr = np.load("my_array.npy")
    # numberOfPathsNext = 0
    # for i in range(len(loaded_arr)):
    #     numberOfPathsNext += len(loaded_arr[i])
    # print("Number of paths: ", len(numberOfPathsNext))
    with open('pickle_test.data', 'wb') as f:
        pickle.dump(all_paths_sorted, f)

    with open('pickle_test.data', 'rb') as f:
        new_data = pickle.load(f)
    numberOfPathsNext = 0
    for i in range(len(new_data)):
        numberOfPathsNext += len(new_data[i])
    print("Number of paths: ", numberOfPathsNext)
    exit()


    elapsed_time = time.time() - start_time
    msg = "\nTrace reading and proc, and also finding all paths from initial to terminal messages took: %s secs (Wall clock time)" % timedelta(milliseconds=round(elapsed_time*1000))
    print(msg)

    res1 = functions.modelrefinement("reducedTrace.txt", all_paths_sorted, selected_paths, graph.myInitialNodes, graph.myTerminalNodes, preFound=specialCounter)
    # res1 = functions.modelrefinement(trace_f[0], all_paths_sorted, selected_paths, graph.myInitialNodes, graph.myTerminalNodes, preFound=0)

    print ("\nAcceptance Ratio = ", res1)
    elapsed_time = time.time() - start_time
    
    msg = "\nIn Total it took: %s secs (Wall clock time)" % timedelta(milliseconds=round(elapsed_time*1000))
    print(msg)
    # print(selected_paths)

    # totalNotAccepted = 0
    # print("Not Accepted")
    # for i in range(len(notAccepted)):
    #     if notAccepted[i]:
    #         print (i, " : ", notAccepted[i])
    #         totalNotAccepted += notAccepted[i]
    # print ("Not accepted in total = ", totalNotAccepted, "\n")

    exit()
